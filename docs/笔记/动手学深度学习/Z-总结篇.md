---
layout: doc
title: 动手学深度学习 Road Map
description: 动手学深度学习 Road Map
date: 2025-03-06
head:
  - - meta
    - name: keywords
      content: 动手学深度学习, 深度学习, 李沐, 动手学大模型
---

# 动手学深度学习 Road Map

[动手学深度学习](https://zh-v2.d2l.ai/)

动手学习深度学习，相信对于 AI 领域的同学来说，并不陌生。在没有接触具体内容之前，我希望可以借鉴到其他同学的一些学习经验，帮助我快速的上手这本书。

> 以下是一些从网上收集来的内容，本人还没有对内容进行验证，请谨慎参考。

## 资料收集

- [:thumbsup: 动手学深度学习](https://zh-v2.d2l.ai/)
- [:robot:《动手学大模型》系列编程实践教程](https://github.com/Lordog/dive-into-llms)

## 网友学习经验摘抄

### 小红书网友

#### 学前基础

1. 编程基础薄弱
2. 应用数学专业
3. C 语言和 Python 基础，无其他编程经验

#### 学习经历

| 阶段         | 内容描述                                                                                   |
| ------------ | ------------------------------------------------------------------------------------------ |
| 学习初期     | 最初，我结合 GPT 学习深度学习，但在学习 RNN 时，感觉自己的代码基础不够，导致无法深入理解。 |
| 学习资源     | 主要观看了 B 站小土堆的教程，他的讲解非常通俗易懂，适合入门学习。                          |
| 深入学习     | 掌握基础知识后，感到自己真正理解了深度学习的模式，重新开始学习李沐的视频课程。             |
| RNN 学习挑战 | 学习 RNN 时感到困难，主要是因为语言数据集处理方法特殊，导致代码处理复杂。                  |
| 学习方法反思 | 最有效的学习方法是自己阅读教材，并结合 GPT 逐步解析内容，完成后再观看李沐的视频。          |
| 遇到的困难   | 对某些内容（如计算性能等）理解不够，感觉缺乏计算机组成原理等相关知识，决定暂时搁置。       |
| 论文阅读     | 阅读了 Transformer 的论文原文，但未能获得新的知识，感觉原文比课本还要简略。                |

#### 对各章节的理解

##### 预备知识

- **基础知识**：具备基础的 Python 和 PyTorch 应用，但在自动微分方面感到困惑。
- **学习建议**：初次接触自动微分时可能会感到迷茫，但不必过于担心，简单了解后对后续学习影响不大。

##### 线性神经网络

- **重要性**：这一章非常重要，帮助理解神经网络处理问题的思路。
- **关键组成部分**：
  - 数据集
  - 模型本身
  - 损失函数
  - 优化算法
- **分类问题**：Softmax 提供了处理分类问题的方法，对图像数据集的理解有助于后续学习卷积神经网络（CNN）。

##### 多层感知机

- **预备知识**：这一章为 CNN 的学习打下基础。
- **基础概念**：学习如何构建更深的网络、实现“非线性”，以及暂退法等。
- **学习难度**：总体来说不难，但需要熟练掌握。

##### 深度学习计算

- **学习状态**：未深入学习，未详细记录。

##### 卷积神经网络

- **学习进展**：逐渐步入正轨，学习卷积神经网络的内容。
- **学习乐趣**：互相关运算的研究过程充满乐趣。

##### 现代卷积神经网络

- **内容概述**：研究现代卷积神经网络，重点在于 AlexNet 和 ResNet。
- **学习重点**：
  - AlexNet 作为奠基者
  - ResNet 作为现代常用网络
  - GoogLeNet 的特殊处理思路

##### 循环神经网络

- **学习逻辑**：按照李沐视频的学习逻辑进行，放在最后学习。
- **学习要点**：
  - 理解序列模型的概念
  - 熟悉语言数据集的构建方法
- **注意事项**：语言数据集的构建与图像数据集有很大差别，需注意输入和输出的形状及各维的含义。

##### 现代循环神经网络

- **重要性**：掌握 GRU 和 LSTM，学习难度不大。
- **概念理解**：深度和双向循环神经网络的概念简单，容易上手。

##### 注意力机制

- **学习内容**：内容接近现代应用，难度适中。
- **学习重点**：熟悉注意力机制的模式，多头注意力和自注意力是 Transformer 的预备知识。

##### 优化算法

- **学习状态**：只听了李沐的课程，整体学习影响不大。

##### 计算性能

- **学习状态**：未深入学习，未详细记录。

##### 计算机视觉

- **应用领域**：卷积神经网络的一个应用，学习难度较大。
- **学习感受**：相对不易，但值得学习，最后的风格迁移应用很有趣。

##### 自然语言处理

- **学习内容**：学习了 BERT 和 BERT 微调，作为 Transformer 的应用。
- **学习感受**：学习过程轻松，现代应用广泛，带来成就感。

####
